// vim: ft=asciidoc

= How to do local development of openshift-tools monitoring components
:toc: macro
:toc-title:

toc::[]


== Description
The purpose of this document is to show how development of openshift-tools monitoring components can be developed locally.

The monitoring components of openshift-tools run inside of containers, so we use containers to develop them as well.

In this document, we're going to focus on the monitoring components, but similar patterns can be used for other components as well.


== Prerequisites

=== Packages
.The following packages are required:
. Docker
. python-ndg_httpsclient-0.3.2-1
. ansible (As of 17 June 2016, Ansible 1.9 must be used, *not* 2.*)

.Fedora:
----
sudo yum -y install docker-io python-ndg_httpsclient-0.3.2-1.fc22.noarch ansible
----


=== Services
.The following services need to be enabled and started:
. docker

.Fedora:
----
sudo systemctl enable docker
sudo systemctl start docker
sudo systemctl status docker
----

== Locating the container definitions (Dockerfiles)
All of the containers are located in the openshift-tools git repository under the `docker` subdirectory.

For a full list of containers, please see the repository https://github.com/openshift/openshift-tools/tree/prod/docker[here].

== Building the monitoring containers
The openshift-tools git repository contains a number of docker containers that are part of our monitoring containers.

Each container has a build.sh script that handles building it locally.

*Note: For local development, you don't need to build all containers, just the ones listed below.*

*Note: To easily build all local development containers, use the `openshift-tools/docker/build-local-setup-centos7.sh` script.*

For the containers to build properly, they must be built in a certain order.

.*Build order*:
. oso-ops-base
. oso-zaio
. oso-zagg-web
. oso-host-monitoring

.Example of building the oso-ops-base container (other containers are similar):
----
> cd openshift-tools/docker/oso-ops-base
> ./generate-containers.yml

[... snip ...]

PLAY [localhost] **************************************************************

GATHERING FACTS ***************************************************************
ok: [localhost]

TASK: [Checking build environment] ********************************************
skipping: [localhost]

TASK: [generate_containers | Clean dirs] **************************************
changed: [localhost]

TASK: [generate_containers | Create dirs] *************************************
changed: [localhost]

TASK: [generate_containers | Copy files] **************************************
changed: [localhost -> 127.0.0.1]

[... snip ...]

TASK: [generate_containers | Insert auto-generated readme] ********************
changed: [localhost]

PLAY RECAP ********************************************************************
localhost                  : ok=15   changed=12   unreachable=0    failed=0


> cd centos7   # You can also choose to build the rhel7 containers
> ./build.sh
[sudo] password for user: *********

Testing sudo works...

[... snip ...]

Building oso-centos7-ops-base...
Sending build context to Docker daemon 27.14 kB
Step 1 : FROM centos:centos7
 ---> 28e524afdd05

[... snip ...]

Successfully built e4f0219b978a
0.06user 0.02system 0:00.55elapsed 15%CPU (0avgtext+0avgdata 27908maxresident)k
0inputs+0outputs (0major+2358minor)pagefaults 0swaps

[... snip ...]

----

=== Caveats

==== AUTOGENERATED content used to build centos7 and rhel7 images
In order to be able to maintain our containers for multiple distros (namely CentOS 7 and RHEL 7), we use a templated build system. The Dockerfile and the contents of the image are autogenerated from the `src` directory to the centos7 and rhel7 directories.

Any changes made directly in the centos7 or rhel7 directories WILL BE OVERWRITTEN the next time the `generate-containers.yml` script is run. This is intentional.

If you want to make persistent changes, they need to be made in the `src` directory inside each container direcotry (e.g. inside the `oso-ops-base` directory). That is what the `generate-containers.yml` script uses to generate the centos7 and rhel7 directories.

When submitting a PR, please make sure to have generated the latest source files, otherwise your PR will be rejected.

==== Build time secrets for RHEL images (DOES NOT APPLY TO CENTOS)
Since the containers are built on RHEL, they expect to have yum credentials setup such that they can yum install packages.

The expected directory structure is outlined https://github.com/twiest/openshift-tools/tree/master/docker#setup[here].

The RHEL7 version of docker contains a secrets patch which copies in the contents of the host's directory /etc/pki/entitlements into the container at the location /run/secrets/etc-pki-entitlement. This copy is done in such a way that it does *NOT* become part of the final image (which is great for build time secrets).

Unfortunately Fedora (and other distros) don't have the secrets patch, so we have a workaround in the build script for oso-rhel7-ops-base that copies the contents of that directory into the image. This means that this docker image contains secrets and thus should *NOT* be shared externally. It should *ONLY* be used for local development.

This also means that any containers built from oso-rhel7-ops-base image should also *NOT* be shared, as they contain the same credentials.

==== Build dependencies
Docker doesn't have container build dependencies, so it is the responsibility of the end user to build the containers in the prescribed order.

==== Stale builds
Docker doesn't automatically build dependent containers, so it is the responsibility of the end user to re-build containers as they get updated in git. It is very difficult to know if a docker file, or any RPMs that a docker file installs have been updated. So, it is recommended that the end user does rebuilds on a regular basis.

All of the build scripts pass additional options down to docker, so normal docker options work.

It is recommended when doing the first build of a container, to pass the `--no-cache` option so that all layers are re-built. This will ensure the container is fully up to date.

.Example build without using the cache
----
> ./build.sh --no-cache
----



== Running the monitoring containers locally
The monitoring containers each include a run.sh script that handles running the container locally. It also handles linking up the containers together, so that they can communicate.

The run.sh scripts run the containers in the foreground. For development purposes, there are a number of advantages.

.Advantages of running containers in the foreground:
. Can use --rm which cleans up the containers after execution
. Can see the output from the run
. Can easily stop the container with ctrl-c


*Note: For local development, you don't need to run all containers, just the ones listed below.*

For the containers to link up properly, they must be run in a certain order. 


*Important:* The oso-zaio container must undergo cleanup and configuration before _oso-zagg-web_ and _oso-host-monitoring_ are run.
Complete everything in the *Setting up the ZAIO* section before running any of the other containers. 

.*Run order*:
. oso-zaio
. _Setup ZAIO_
. oso-zagg-web
. oso-host-monitoring

.Example of running the oso-zaio centos7 container (other containers are similar):
----
> cd openshift-tools/docker/oso-zaio/centos7
> ./run.sh

Testing sudo works...

Running zaio...
Preparing the db

[... snip ...]

----

*Note: the container will stay in the foreground (on purpose). This allows you to see the container startup messages and allows you to exit the container by pressing ctrl-c*

== Setting up the ZAIO
The ZAIO is a completely default installation of zabbix. It has the default templates, items and triggers loaded. The reason it's completely default is that the ZAIO is used by multiple teams, for multiple projects. Therefore, having a default install makes sense.

The ZAIO is also a completely ephemeral. Meaning that when the ZAIO container is stopped and then re-started, it is back to being a completely default installation of zabbix. This is great for development.

This makes the ZAIO a great resource for local development.

=== Setting up the openshift-tools Ansible Zabbix Modules
In order to setup the openshift-tools Ansible Zabbix modules, you need to either install the Zabbix Ansible Module package, or setup the python path to point to the openshift_tools python module.

For development purposes, it's almost always preferable to set the python path, as then you're guaranteed to have the latest code.

.Steps to configure the Ansible Zabbix Modules:
. Change into the openshift-tools git repo
[source]
----
> cd openshift-tools/ansible/playbooks/adhoc/zabbix_setup
----
. Set the python path to see our openshift_tools python package, as well as the system's site-packages
[source]
----
> export PYTHONPATH=$(pwd)/../../../..:$(ls /usr/lib/python2*/site-packages/ -d)
----


=== Cleaning out the ZAIO
For our monitoring development, we really don't need the vast majority of the default templates.

To clean them out, run the `oo-clean-zaio.yml` playbook.

.Example of running the oo-clean-zaio.yml playbook:
----
> ./oo-clean-zaio.yml
----

=== Setup the Monitoring templates, items and triggers in the ZAIO
After cleaning out the default templates, items and triggers from the ZAIO, it is necessary to setup the ZAIO with our latest monitoring templates, items and triggers.

To do this, run the `oo-config-zaio.yml` playbook.

*Note: this playbook is idempotent and can be run multiple times without any new changes being made.*

.Example of running the oo-config-zaio.yml playbook:
----
> ./oo-config-zaio.yml
----

== Logging into the ZAIO
Once the containers are up and running, you can log into the Zaio http://localhost/zabbix[here].

The username is `Admin`.
The password is `zabbix`.

== Container Features

=== /shared
All of the containers when run locally mount /var/lib/docker/volumes/shared into the container on /shared. This makes it really easy to share data between containers during development.

*Note: /shared is NOT available when run inside of OpenShift because these containers can (and probably do) run on different machines in the cluster. So don't rely on it for container features.*

All of the containers run with the --rm flag, which means that they when they exit, they clean up all of their resources. Therefore, it is recommended that you store any changes made in /shared so that they are persistent.

In fact, the common development pattern is to do a git clone right inside of /shared and develop in git directly and then symlink from the container into the git repo.

== Testing the local setup
To test the local setup, send a heartbeat from oso-host-monitoring and make sure it shows up in zabbix.

=== Send a heartbeat from the oso-host-monitoring centos7 container:

.Here are the steps:
. Enter the oso-host-monitoring centos7 container:
+
----
> sudo docker exec -ti oso-centos7-host-monitoring bash
----
+
. Inside the container send a heartbeat:
+
----
[CTR][root@ ~]$ /usr/bin/ops-zagg-client --send-heartbeat
----
+
. Exit the oso-centos7-host-monitoring container:
+
----
[CTR][root@ ~]$ exit
----

=== Force oso-centos7-zabbix-web to send the heartbeat to zabbix immediately:

.Here are the steps:
. Enter the oso-centos7-zagg-web container:
+
----
> sudo docker exec -ti oso-centos7-zagg-web bash
----
+
. Inside the container send a heartbeat:
+
----
[CTR][root@ ~]$ ops-zagg-processor
----
+
. Exit the oso-centos7-zagg-web container:
+
----
[CTR][root@ ~]$ exit
----

=== Check that the heartbeat was sent to the ZAIO

.Ensure the heartbeat made it to the ZAIO:
. Check that the host exists in the ZAIO http://localhost/zabbix/hosts.php[here].
. Check latest data
.. Go to the latest data page http://localhost/zabbix/latest.php[here].
.. Click "Show filter"
.. Click the "Select" button next to "Hosts"
.. Select your host
.. Click the "Filter" button
.. Expand the plus sign next to "- other -"
.. Look for the "Heartbeat Ping" item and note if the value is set or not.
.. If the value is set, your local development environment has been successfully setup.

== Conclusion
Your machine should now be setup for local container development.
