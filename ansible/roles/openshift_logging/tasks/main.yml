---
- name: get oc version
  oc_version:
  register: oc_version

- name: set the kube version
  set_fact:
    kube_version: "{{ oc_version.result.kubernetes_short }}"

- name: get cert's basename
  set_fact:
    cert_basename: "{{ osalog_default_logging_cert | basename }}"
    key_basename: "{{ osalog_default_logging_key | basename }}"
    cacert_basename: "{{ osalog_default_logging_cacert | basename }}"

- name: copy logging certs to masters
  copy:
    src: "{{ item }}"
    dest: /etc/origin/master/named_certificates/
  with_items:
  - "{{ osalog_default_logging_cert }}"
  - "{{ osalog_default_logging_key }}"
  - "{{ osalog_default_logging_cacert }}"

- name: create project
  oadm_project:
    name: logging
    display_name: logging
  register: projectout
  run_once: true
- debug: var=projectout

- name: Allow logging to deploy to all node types
  oc_edit:
    kind: ns
    name: logging
    separator: '#'
    content:
      metadata#annotations#openshift.io/node-selector: ""

- name: create secret
  oc_secret:
    namespace: logging
    name: logging-deployer
    files:
    - name: kibana.crt
      path: "/etc/origin/master/named_certificates/{{ cert_basename }}"
    - name: kibana.key
      path: "/etc/origin/master/named_certificates/{{ key_basename }}"
  register: secretout
  run_once: true
- debug: var=secretout

- name: Generate logging-deployer configmap
  template:
    src: logging-deployer.yaml.j2
    dest: /tmp/logging-deployer.yaml

- name: Create logging-deployer configmap
  command: oc create -f /tmp/logging-deployer.yaml -n logging

- name: create logging deployer template
  oc_obj:
    state: present
    namespace: logging
    name: logging-deployer-template
    kind: template
    files:
    - "/usr/share/ansible/openshift-ansible/roles/openshift_examples/files/examples/v{{ kube_version }}/infrastructure-templates/enterprise/logging-deployer.yaml"
  register: templateout
  run_once: true
- debug: var=templateout

- name: create template
  oc_process:
    namespace: logging
    template_name: logging-deployer-account-template
    create: True
    reconcile: False
  register: processout
  run_once: true
- debug: var=processout

- name: create role binding to logging-deployer service account
  oadm_policy_user:
    namespace: logging
    user: system:serviceaccount:logging:logging-deployer
    resource_kind: role
    resource_name: edit
  register: policyout
  run_once: true
- debug: var=policyout

- name: add cluster role binding to logging-deployer service account
  oadm_policy_user:
    namespace: logging
    user: system:serviceaccount:logging:logging-deployer
    resource_kind: cluster-role
    resource_name: oauth-editor
  register: policyout
  run_once: true
- debug: var=policyout

- name: add scc to aggregated-logging-fluentd service account
  oadm_policy_user:
    namespace: logging
    user: system:serviceaccount:logging:aggregated-logging-fluentd
    resource_kind: scc
    resource_name: privileged
  register: policyout
  run_once: true
- debug: var=policyout

- name: add cluster role binding to aggregated-logging-fluentd service account
  oadm_policy_user:
    namespace: logging
    user: system:serviceaccount:logging:aggregated-logging-fluentd
    resource_kind: cluster-role
    resource_name: cluster-reader
  register: policyout
  run_once: true
- debug: var=policyout

- name: create template
  oc_process:
    namespace: logging
    template_name: logging-deployer-template
    create: True
    reconcile: False
  register: processout
  run_once: true
- debug: var=processout

# wait 3 minutes for deployer pod to complete
- name: wait until logging deployer pod has completed
  oc_obj:
    state: list
    namespace: logging
    kind: pods
    selector: logging-infra=deployer
  register: podout
  until: podout.results.results[0]['items'][0].status.phase == 'Succeeded'
  retries: 36
  delay: 5
  run_once: true

- name: get deployer pod status
  oc_obj:
    state: list
    namespace: logging
    kind: pods
    selector: logging-infra=deployer
  register: podout
  run_once: true

- fail:
    msg: logging deployer failed to complete setup
  when: podout.results.results[0]['items'][0].status.phase != 'Succeeded'
  run_once: true

- name: get deployer pod status
  oc_obj:
    state: list
    namespace: logging
    kind: pods
  register: podout
  run_once: true

- name: fetch the elastic search dc's with selector
  oc_obj:
    state: list
    namespace: logging
    kind: dc
    selector: logging-infra=elasticsearch
  register: dcout
  run_once: true
- debug: var=dcout

- name: attach storage volumes to instances
  oc_pvc:
    namespace: logging
    name: "{{ item['metadata']['name'] }}"
    access_modes:
    - ReadWriteOnce
    volume_capacity: "{{ osalog_pv_size }}G"
  with_items: "{{ dcout.results.results[0]['items'] }}"
  register: pvcout
  run_once: true
- debug: var=pvcout

- name: wait until logging deployer pod has completed
  oc_obj:
    state: list
    namespace: logging
    kind: pvc
  register: pvcout
  until:
  - pvcout.results.results[0]['items'][0].status.phase == 'Bound'
  - pvcout.results.results[0]['items'][1].status.phase == 'Bound'
  retries: 24
  delay: 5
  run_once: true

- name: attach storage volumes to instances
  oc_volume:
    namespace: logging
    kind: dc
    name: "{{ item['metadata']['name'] }}"
    mount_type: pvc
    claim_name: "{{ item['metadata']['name'] }}"
    claim_size: "{{ osalog_pv_size }}G"
    vol_name: elasticsearch-storage
  with_items: " {{ dcout.results.results[0]['items'] }}"
  register: volumeout
  run_once: true
- debug: var=volumeout

- name: Remove old logging-fluentd configmap
  command: oc delete configmap/logging-fluentd -n logging

- name: Generate logging-fluentd configmap
  template:
    src: logging-fluentd.yaml.j2
    dest: /tmp/logging-fluentd.yaml

- name: Create logging-fluentd configmap
  command: oc create -f /tmp/logging-fluentd.yaml -n logging

- name: scale fluentd
  oc_label:
    selector: "{{ item }}"
    state: add
    kind: node
    labels:
      - key: logging-infra-fluentd
        value: 'true'
  with_items: " {{ osalog_fluentd_nodes }}"

- name: scale kibana frontend
  oc_scale:
    name: logging-kibana
    kind: dc
    namespace: logging
    replicas: 2
  register: scaleout
  run_once: true
- debug: var=scaleout

- name: add kibana url to master-config.yml
  yedit:
    src: /etc/origin/master/master-config.yaml
    key: assetConfig.loggingPublicURL
    value: "https://logs.{{ osalog_clusterid }}.openshift.com"
  register: yeditout
  notify:
  - restart openshift master services
- debug: var=yeditout

- name: Remove old logging-curator configmap
  command: oc delete configmap/logging-curator -n logging

- name: Generate logging-curator configmap
  template:
    src: logging-curator.yaml.j2
    dest: /tmp/logging-curator.yaml

- name: Create logging-curator configmap
  command: oc create -f /tmp/logging-curator.yaml -n logging

- name: Add the resource constraints to the curator
  oc_edit:
    kind: dc
    name: logging-curator
    namespace: logging
    content:
      spec.template.spec.containers[0].resources.limits.memory: "2G"
      spec.template.spec.containers[0].resources.requests.memory: "1G"

- name: Redeploy curator
  command: oc deploy --latest dc/logging-curator -n logging

- name: Add the resource constraints to the metrics RC
  oc_edit:
    kind: dc
    name: logging-kibana
    namespace: logging
    content:
      # kibana
      spec.template.spec.containers[0].resources.limits.memory: "768M"
      spec.template.spec.containers[0].resources.requests.memory: "96M"
      # kibana-proxy
      spec.template.spec.containers[1].resources.limits.memory: "128M"
      spec.template.spec.containers[1].resources.requests.memory: "32M"

- name: Redeploy kibana
  command: oc deploy dc/logging-kibana --latest -n logging

#  We may need a reencrypt route in the future
#  This was tested, but didn't quite work.
#
#- name: create route for the kibana
#  oc_route:
#    name: kibana
#    namespace: logging
#    tls_termination: reencrypt
#    cert_path: "/etc/origin/master/named_certificates/{{ cert_basename }}"
#    key_path: "/etc/origin/master/named_certificates/{{ key_basename }}"
#    cacert_path: "/etc/origin/master/named_certificates/{{ cacert_basename }}"
#    dest_cacert_path: "/etc/origin/master/named_certificates/{{ cacert_basename }}"
#    service_name: logging-kibana
#    host: "logs.{{ osalog_clusterid }}.openshift.com"
#  register: routeout
#  run_once: True
#
#- debug: var=routeout
#  run_once: True
